{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3caf6c58-a167-4a5f-a159-ea92be1c023e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Images, rich with untapped information, often come under the radar of search engines and data systems. Transforming this visual data into machine-readable language is no easy task, but it's where image captioning AI is useful. Here's how image captioning AI can make a difference:\n",
    "\n",
    "Improves accessibility: Helps visually impaired individuals understand visual content.\n",
    "\n",
    "\n",
    "Enhances SEO: Assists search engines in identifying the content of images.\n",
    "\n",
    "\n",
    "Facilitates content discovery: Enables efficient analysis and categorization of large image databases.\n",
    "\n",
    "\n",
    "Supports social media and advertising: Automates engaging description generation for visual content.\n",
    "\n",
    "\n",
    "Boosts security: Provides real-time descriptions of activities in video footage.\n",
    "\n",
    "\n",
    "Aids in education and research: Assists in understanding and interpreting visual materials.\n",
    "\n",
    "\n",
    "Offers multilingual support: Generates image captions in various languages for international audiences.\n",
    "\n",
    "\n",
    "Enables data organization: Helps manage and categorize large sets of visual data.\n",
    "\n",
    "\n",
    "Saves time: Automated captioning is more efficient than manual efforts.\n",
    "\n",
    "\n",
    "Increases user engagement: Detailed captions can make visual content more engaging and informative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f6e3a-4bb5-4a28-9b25-71fa06b59aaf",
   "metadata": {},
   "source": [
    "Overview of BLIP and Hugging Face Transformers\n",
    "Hugging Face Transformers Library:\n",
    "\n",
    "An open-source library that provides state-of-the-art models for various Natural Language Processing (NLP) tasks, such as text classification, translation, and more.\n",
    "It supports multimodal learning, integrating both text and image data for tasks like image captioning and visual question answering.\n",
    "\n",
    "BLIP Model:\n",
    "\n",
    "Purpose: Designed to improve the understanding and generation of image descriptions by associating images with relevant text.\n",
    "Applications: Generating captions, answering questions related to images, and enhancing search queries using images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049a15ac-d4d6-468d-aa55-46e64d7af65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers Pillow torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511ea5aa-e226-4ba1-8644-84c327b49ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\raman\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raman\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raman\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\raman\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\raman\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52a4db-ebb9-469a-a7da-4c102ec2e24a",
   "metadata": {},
   "source": [
    "#pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9faa2650-4bbd-4b15-899b-ed5745cf9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d57b4b-df27-4f40-90fe-2329b4e9c2b2",
   "metadata": {},
   "source": [
    "Now The code snippet you provided is initializing a processor and model from Hugging Face's transformers library for image captioning using the BLIP model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2202d7-0bb5-4620-8ecc-862474ea9adf",
   "metadata": {},
   "source": [
    "## Conditional Image Captioning\n",
    "In this process, we leverage a model to generate captions for images based on specific input text. The key steps are:\n",
    "\n",
    "# Input Handling:\n",
    "\n",
    "URL Image: Allows fetching images from the web dynamically.\n",
    "Raw Image: Enables using locally stored images for testing.\n",
    "\n",
    "# Preprocessing:\n",
    "\n",
    "The processor (e.g., BlipProcessor) prepares the image and text by resizing, normalizing, and tokenizing them. This ensures compatibility with the model.\n",
    "Contextual Guidance:\n",
    "\n",
    "By providing a text prompt (like \"a photography of\"), we guide the model to generate contextually relevant captions.\n",
    "Model Operation:\n",
    "\n",
    "The model processes the inputs and generates a caption based on the provided image and text.\n",
    "This workflow helps produce meaningful captions tailored to the specified context while allowing flexibility in image sourcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0f80a6-60ad-438c-beff-1e921cde1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "Processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ecc684-cb03-4061-a258-141dc7750f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raman\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the processor and model\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b26469-e1f7-4f46-9f6f-a269f01c7ae0",
   "metadata": {},
   "source": [
    "## Visual Question Answering\n",
    "To perform Visual Question Answering (VQA) using the BLIP (Bootstrapping Language-Image Pre-training) model, you'll need to set up a few things. Hereâ€™s a basic example of how you can implement this in Python using the Hugging Face Transformers library:\n",
    "\n",
    "Explanation: BLIP Processor: The processor handles the preprocessing of the input images and questions.\n",
    "\n",
    "BLIP Model: The model predicts the answer based on the image and the question. Evaluation Mode: Setting the model to evaluation mode disables dropout layers, which is essential for inference. Answering the Question: The answer_question function takes an image path and a question, processes them, and returns the predicted answer.\n",
    "\n",
    "Note: Make sure to replace \"path/to/your/image.jpg\" with the actual path to the image you want to analyze. Ensure your system has the necessary hardware and libraries installed to run the model, especially if you want to leverage GPU for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a4d5e36-84b5-41f5-9b25-86bce6b45d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a man is holding a white dog by the water\n"
     ]
    }
   ],
   "source": [
    "# Initialize the processor and model from Hugging Face\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "# Load an image\n",
    "image = Image.open(r\"C:\\Users\\Raman\\OneDrive\\Desktop\\smiling-asian-man-playing-lovely-600nw-1960043986.webp\")\n",
    "# Prepare the image\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "# Generate captions\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0],skip_special_tokens=True)\n",
    " \n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0340bf3-484c-4ba7-8466-afa032f9c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f95d724-04f9-4c28-bdd5-66f4ae5cedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c26a93-daa3-4ef6-81cf-e50fdd25ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raman\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a woman and her dog on the beach\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cda564f-63ec-480d-8b07-aeaa73e25f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a942a180-87a8-4346-a50e-82f9d32a8439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman sitting on the beach with her dog and a cell phone\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16630335-0ba4-48e5-bd02-3e5637e43249",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "BLIP from Hugging Face Transformers opens new possibilities for AI applications by enabling a deeper understanding of visual content and textual descriptions. Using BLIP, developers and researchers can create more intuitive, accessible, and engaging applications that bridge the gap between the visual world and natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932e8cc-9786-422b-a594-52b65f432165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
